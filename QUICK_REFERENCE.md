# UML Sequence Diagram - Quick Reference Card

## ğŸ¯ At a Glance

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           LOG ANALYZER APP - EXECUTION FLOW                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  START                                                       â”‚
â”‚    â†“                                                         â”‚
â”‚  1ï¸âƒ£ UPLOAD FILE â†’ 2ï¸âƒ£ VALIDATE â†’ 3ï¸âƒ£ DECODE                 â”‚
â”‚    â†“                                                         â”‚
â”‚  4ï¸âƒ£ INITIALIZE ANALYZER                                     â”‚
â”‚    â†“                                                         â”‚
â”‚  5ï¸âƒ£ INGEST LOG (Split â†’ Embed â†’ Store in Pinecone)          â”‚
â”‚    â†“                                                         â”‚
â”‚  6ï¸âƒ£ READY FOR ANALYSIS                                      â”‚
â”‚    â”œâ”€ SUMMARIZE (Generate summary)                          â”‚
â”‚    â””â”€ Q&A (Answer questions via RAG)                        â”‚
â”‚    â†“                                                         â”‚
â”‚  END / UPLOAD NEW FILE                                      â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“‹ Phase Checklist

| # | Phase | Actor | Action | Duration |
|---|-------|-------|--------|----------|
| 1 | Initialization | UI | Load config & session state | <1s |
| 2 | Upload | User | Select .log file | User-dependent |
| 3 | Validation | Validator | Check extension & size | <1s |
| 4 | Decoding | UI | Convert bytes to text | ~1s |
| 5 | Analyzer Init | LLMAnalyzer | Create instance & connect | ~2s |
| 6 | Ingestion | Analyzer | Split, embed, store chunks | 5-30s |
| 7a | Summarization | Analyzer | Generate summaries | 10-60s |
| 7b | Q&A | Analyzer | Retrieve & answer | 5-15s |

---

## ğŸ”„ Key Interactions

### Interaction 1: File Validation
```
User â”€â”€(upload file)â”€â”€> UI â”€â”€(validate)â”€â”€> FileValidator
                                               â†“
                          (valid?) â”€â”€(yes)â”€â”€> Continue
                          (valid?) â”€â”€(no)â”€â”€> Show Error â†’ STOP
```

### Interaction 2: Analyzer Initialization
```
UI â”€â”€(check session)â”€â”€> Session
                          â†“
                   (exists?) â”€â”€(yes)â”€â”€> Use cached analyzer
                   (exists?) â”€â”€(no)â”€â”€> Create new
                                          â”œâ”€ Init Pinecone
                                          â”œâ”€ Init LLM
                                          â””â”€ Init Embeddings
```

### Interaction 3: Log Ingestion
```
UI â”€â”€(ingest text)â”€â”€> Analyzer â”€â”€> LangChain
                         â”œâ”€ Split chunks
                         â””â”€â”€> Pinecone
                              â”œâ”€ Delete old index
                              â”œâ”€ Create new index
                              â””â”€ Add chunks
                              
Status: skip_ingest = True (prevent re-run)
```

### Interaction 4: Summarization
```
UI â”€â”€(summarize)â”€â”€> Analyzer â”€â”€> LangChain
                      â”œâ”€ Chunk summaries (loop 5x)
                      â”œâ”€ LLM invoke per chunk
                      â””â”€ Final summary generation
                      
Status: skip_summary = True (prevent re-run)
```

### Interaction 5: RAG Analysis
```
UI â”€â”€(question)â”€â”€> Analyzer â”€â”€> Pinecone
                      â”œâ”€ Vector search (top 10)
                      â””â”€â”€> LangChain
                           â”œâ”€ Create retriever
                           â”œâ”€ Build context
                           â””â”€ LLM generation
```

---

## ğŸ¨ Component Roles

### **User** ğŸ‘¤
- Uploads .log file
- Clicks "Summarize" button
- Enters questions
- Views results

### **Streamlit UI** ğŸ–¥ï¸
- Displays interface
- Manages widgets
- Handles file upload
- Shows results
- Manages session state

### **FileValidator** âœ…
- Validates file extension (.log)
- Checks file size (â‰¤ 100 MB)
- Returns validation result

### **LLMVectorAnalyzer** ğŸ¤–
- Orchestrates entire analysis
- Manages Pinecone connection
- Handles text splitting
- Coordinates LLM calls
- Three main methods:
  - `ingest_log_langchain_llm()` - Store embeddings
  - `summarize()` - Generate summaries
  - `analyze_log_rag()` - Answer questions

### **Pinecone** ğŸ“¦
- Vector database
- Stores embeddings
- Similarity search
- Index management

### **LangChain** â›“ï¸
- LLM orchestration
- Text splitting
- Chain creation
- Prompt management
- Ollama models:
  - ChatOllama (gemma2:latest)
  - OllamaEmbeddings (mxbai-embed-large:335m)

### **Session State** ğŸ’¾
- Caches analyzer instance
- Tracks skip_ingest flag
- Tracks skip_summary flag
- Preserves state across reruns

---

## ğŸ”€ Control Flow Decisions

### Decision Point 1: File Validation
```
Is file valid?
â”œâ”€ YES â†’ Continue to decoding
â””â”€ NO â†’ Show error, STOP
```

### Decision Point 2: Analyzer in Session
```
Does analyzer exist in session?
â”œâ”€ YES â†’ Use cached instance
â””â”€ NO â†’ Create new instance
```

### Decision Point 3: Skip Ingestion?
```
Has ingestion been done?
â”œâ”€ YES (skip_ingest=True) â†’ Skip ingestion
â””â”€ NO (skip_ingest=False) â†’ Run ingestion, set flag
```

### Decision Point 4: Skip Summarization?
```
Has summary been generated?
â”œâ”€ YES (skip_summary=True) â†’ Skip summarization
â””â”€ NO (skip_summary=False) â†’ Run summary, set flag
```

---

## ğŸ“Š Data Structures

### File Upload
```python
{
    filename: str,
    size: int (bytes),
    content: bytes
}
```

### Validation Result
```python
(ok: bool, message: str | None)
```

### Analyzer
```python
LLMVectorAnalyzer(
    openai_api_key: str,
    pinecone_api_key: str,
    index_name: str
)
```

### Chunks
```python
[chunk1: str, chunk2: str, ...]
(max 95 per batch for ingestion)
```

### Summaries
```python
[summary1: str, summary2: str, ...]
(max 5 chunks for summarization)
```

### RAG Response
```python
{
    answer: str,
    source_documents: [doc1, doc2, ...]
}
```

---

## âš™ï¸ Configuration

### Environment Variables (.env)
```
OPENAI_API_KEY=sk-...
PINECONE_API_KEY=...
```

### Application Constants
```
index_name = "index-log"
max_file_size = 100 MB
chunk_size_ingest = 50 chars
chunk_size_summary = 1000 chars
max_chunks_summary = 5
rag_k = 10  # top-k retrieval
```

### LLM Configuration
```
LLM: ChatOllama
  model: "gemma2:latest"
  temperature: 0

Embeddings: OllamaEmbeddings
  model: "mxbai-embed-large:335m"
```

### Pinecone Index
```
Spec: ServerlessSpec
  cloud: "aws"
  region: "us-east-1"
dimension: 1024
```

---

## ğŸ›¡ï¸ Error Handling

```
Try Block 1: File Validation
  â””â”€ Catch: Show error message

Try Block 2: File Decoding
  â””â”€ Except UTF-8: Try Latin-1

Try Block 3: Log Ingestion
  â””â”€ Catch Exception: Log + Show error

Try Block 4: Summarization
  â””â”€ Catch Exception: Log + Show error

Try Block 5: RAG Analysis
  â””â”€ Catch Exception: Log + Show error
```

---

## ğŸ“ˆ Sequence Timing

```
Timeline (with typical durations):

T+0s     Load app, initialize session state
T+0.5s   Display UI
T+1s     User uploads file (~varies)
T+2s     Validation & decoding complete
T+4s     Analyzer initialized (if new)
T+34s    Ingestion complete (if large file)
T+35s    Ready for summarization/Q&A
T+95s    Summarization complete (if clicked)
T+110s   Or Q&A responses ready
```

---

## ğŸ”— Method Call Chain

### Ingestion Chain
```
UI.ingest_log_langchain_llm(text)
  â”œâ”€ RecursiveCharacterTextSplitter.split_text()
  â”œâ”€ Pinecone.has_index()
  â”œâ”€ Pinecone.delete_index()
  â”œâ”€ Pinecone.create_index()
  â””â”€ PineconeVectorStore.add_texts()
```

### Summarization Chain
```
UI.summarize(text)
  â”œâ”€ Analyzer.summarize_chunks(text)
  â”‚   â”œâ”€ RecursiveCharacterTextSplitter.split_text()
  â”‚   â””â”€ for each chunk[0:5]:
  â”‚       â””â”€ prompt_template_chunk_summary | llm | parser
  â””â”€ prompt_template_summary | llm | parser
```

### RAG Analysis Chain
```
UI.analyze_log_rag(prompt)
  â”œâ”€ Pinecone.Index()
  â”œâ”€ PineconeVectorStore.as_retriever()
  â”œâ”€ create_stuff_documents_chain()
  â”œâ”€ create_retrieval_chain()
  â””â”€ rag_chain.invoke()
      â”œâ”€ retriever.get_relevant_documents()
      â””â”€ llm.invoke(context + prompt)
```

---

## ğŸ“ Learning Path

**Beginner**: Start with ASCII diagrams
â†’ Quick understanding of overall flow

**Intermediate**: Read SEQUENCE_DIAGRAM_DOCS.md
â†’ Understand each phase in detail

**Advanced**: Study the PlantUML diagram
â†’ Formal UML notation and interactions

**Expert**: Read app.py code + diagrams
â†’ Full implementation details

---

## ğŸ“± Mobile/UX Considerations

```
Phase 1: Upload (~1 second)
  â””â”€ Simple file picker

Phase 2: Processing (5-30 seconds)
  â””â”€ Show spinner + "Ingesting log"

Phase 3: Results (interactive)
  â”œâ”€ Summary button
  â””â”€ Question input field

Phase 4: Streaming Results
  â””â”€ Display results in container
```

---

## ğŸ” Security Flow

```
1. Load env vars (not hardcoded keys)
2. Validate file (extension + size)
3. Decode safely (UTF-8 â†’ Latin-1)
4. Process in sandbox (LangChain isolation)
5. Store securely (Pinecone encryption)
6. Return sanitized results
```

---

## ğŸ“š File Reference Guide

| File | Purpose | Read Time |
|------|---------|-----------|
| `app_sequence_diagram.puml` | Formal UML diagram | 10 min |
| `SEQUENCE_DIAGRAM_DOCS.md` | Detailed explanation | 20 min |
| `FLOW_DIAGRAM_ASCII.md` | Visual ASCII flows | 10 min |
| `UML_DIAGRAMS_README.md` | Complete overview | 10 min |
| **This file** | **Quick reference** | **5 min** |

---

## ğŸ¯ Quick Answers

**Q: How long does ingestion take?**
A: 5-30 seconds depending on file size

**Q: Can I ask multiple questions?**
A: Yes! Once ingested, ask unlimited questions

**Q: What formats are supported?**
A: Only .log files (max 100 MB)

**Q: Is data stored permanently?**
A: Only in Pinecone index (session-specific)

**Q: Can I re-summarize?**
A: No (skip_summary flag prevents it)

**Q: How many chunks are processed for summary?**
A: Maximum 5 chunks

**Q: What's the RAG retrieval size?**
A: Top 10 most similar chunks

---

**Quick Reference Card v1.0**
**Generated**: January 26, 2026
**Format**: Markdown
**Status**: âœ… Complete
